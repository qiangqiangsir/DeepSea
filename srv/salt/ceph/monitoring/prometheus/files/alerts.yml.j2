groups:
  - name: cluster health
    rules:
      - alert: health error
        expr: ceph_health_status == 2
        for: 5m
        annotations:
          description: Ceph in error for > 5m
          severity: critical
      - alert: unhealthy
        expr: ceph_health_status != 0
        for: 15m
        annotations:
          description: Ceph not healthy for > 5m
          severity: warning
  - name: mon
    rules:
      - alert: low monitor quorum count
        expr: ceph_monitor_quorum_count < 3
        annotations:
          description: Monitor count in quorum is low
          severity: critical
  - name: osd
    rules:
      - alert: 10% OSDs down
        expr: sum(ceph_osd_down) / count(ceph_osd_in) >= 0.1
        annotations:
          description: More then 10% of OSDS are down
          severity: critical
      - alert: OSD down
        expr: sum(ceph_osd_down) > 1
        for: 15m
        annotations:
          description: One or more OSDS down for more then 15 minutes
          severity: warning
      - alert: OSDs near full
        expr: (ceph_osd_utilization unless on(osd) ceph_osd_down) > 80
        annotations:
          description: OSD {{ $labels.osd }} is dangerously full, over 80%
          severity: ciritcal
      # alert on high commit latency...but how high is too high
  - name: mds
    rules:
  - name: mgr
    rules:
  - name: nodes
    rules:
      alert: root volume filling up
      expr: node_filesystem_avail{mountpoint="/"} / node_filesystem_size{mountpoint="/"} < 0.1
      annotations:
        description: Root volume (OSD and MON store) is filling up (< 10% free)
        severity: critical
